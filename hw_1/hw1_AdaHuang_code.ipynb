{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CSCE 633-600 Homework 1**\n",
    "\n",
    "**Problem 1: Cosine and Dot Product Similarity**\n",
    "\n",
    "Ada Huang\n",
    "\n",
    "January 25, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using dic to store the words in the corpus\n",
    "dic = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the folder to get each file's content in docs\n",
    "file_list = os.listdir('docs')\n",
    "file_contents = []\n",
    "\n",
    "# iterate through the folder to get each file's content\n",
    "for file in file_list:\n",
    "    file_path = os.path.join('docs', file)\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            content = f.read()\n",
    "            file_contents.append(content)\n",
    "            for word in content.split():\n",
    "                dic.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the folder queries to get each file's content in query\n",
    "query_list = os.listdir('queries')\n",
    "query_contents = []\n",
    "\n",
    "# iterate through the folder to get each file's content\n",
    "for file in query_list:\n",
    "    file_path = os.path.join('queries', file)\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            content = f.read()\n",
    "            query_contents.append(content)\n",
    "            for word in content.split():\n",
    "                dic.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of all words in corpus are 2869\n"
     ]
    }
   ],
   "source": [
    "print('the number of all words in corpus are', len(dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all contents in corpus\n",
    "all_contents = file_contents + query_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vocab of all unique words in all the files\n",
    "# using token_pattern to not split words with punctuations and apostrophes in the middle\n",
    "pattern = \"\\\\b[\\\\w/'-]+\\\\b\"\n",
    "vectorizer = CountVectorizer(token_pattern=pattern)\n",
    "vocab = vectorizer.fit_transform(all_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_the</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aan</th>\n",
       "      <th>aas</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abort</th>\n",
       "      <th>abs</th>\n",
       "      <th>absence</th>\n",
       "      <th>absolute</th>\n",
       "      <th>...</th>\n",
       "      <th>yanks</th>\n",
       "      <th>yep</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youth</th>\n",
       "      <th>zip</th>\n",
       "      <th>zip/unzip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2869 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   _the  aaa  aan  aas  ability  able  abort  abs  absence  absolute  ...  \\\n",
       "0     0    0    0    0        0     0      0    0        0         0  ...   \n",
       "1     0    0    0    0        0     0      0    0        0         0  ...   \n",
       "2     0    0    0    0        0     0      0    0        0         0  ...   \n",
       "3     0    0    0    0        0     0      0    0        0         0  ...   \n",
       "4     0    0    0    0        0     1      0    0        0         0  ...   \n",
       "\n",
       "   yanks  yep  yes  yesterday  york  young  younger  youth  zip  zip/unzip  \n",
       "0      0    0    1          0     0      0        0      0    0          0  \n",
       "1      0    0    0          0     0      0        0      0    0          0  \n",
       "2      0    0    0          0     0      0        0      0    0          0  \n",
       "3      0    0    0          0     0      0        0      0    0          0  \n",
       "4      0    0    0          0     0      0        0      0    0          0  \n",
       "\n",
       "[5 rows x 2869 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view full vocab as a dataframe\n",
    "df_all = pd.DataFrame(vocab.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using file name and query name as index\n",
    "df_all.index = file_list + query_list\n",
    "# df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of vocabs are 2869 , which are the same as the number of words in the corpus.\n"
     ]
    }
   ],
   "source": [
    "print('the total number of vocabs are', df_all.shape[1], ', which are the same as the number of words in the corpus.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = df_all.index.tolist()\n",
    "# index_list\n",
    "index_list.remove('1')\n",
    "index_list.remove('2')\n",
    "index_list.remove('3')\n",
    "index_list.remove('4')\n",
    "index_list.remove('5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the dot product of each doc with query and show the top 10\n",
    "ans = []\n",
    "for i in range(1, 6):\n",
    "    q = df_all.loc[str(i)]    \n",
    "    dot_product = []\n",
    "    for doc in index_list:\n",
    "        doc_vector = df_all.loc[doc]\n",
    "        dot_prod = np.dot(q, doc_vector)\n",
    "        dot_product.append((dot_prod, doc))\n",
    "    dot_product.sort(reverse=True)\n",
    "    top10_dot = dot_product[:10]\n",
    "    ans.append(top10_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the cosine similarity of each doc with query and show the top 10\n",
    "ans2 = []\n",
    "for i in range(1, 6):\n",
    "    q = df_all.loc[str(i)]    \n",
    "    cosine_sim = []\n",
    "    for doc in index_list:\n",
    "        doc_vector = df_all.loc[doc]\n",
    "        cos_sim = round(np.dot(q, doc_vector) / (np.linalg.norm(q) * np.linalg.norm(doc_vector)), 3)\n",
    "        cosine_sim.append((cos_sim, doc))\n",
    "    cosine_sim.sort(reverse=True)\n",
    "    top10_cos = cosine_sim[:10]\n",
    "    ans2.append(top10_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(170, '51060'),\n",
       "  (21, '49960'),\n",
       "  (11, '51153'),\n",
       "  (10, '51165'),\n",
       "  (9, '51164'),\n",
       "  (7, '51144'),\n",
       "  (7, '51120'),\n",
       "  (6, '51135'),\n",
       "  (6, '51130'),\n",
       "  (5, '51161')],\n",
       " [(64, '59905'),\n",
       "  (43, '60103'),\n",
       "  (30, '59850'),\n",
       "  (19, '59874'),\n",
       "  (17, '59873'),\n",
       "  (15, '59909'),\n",
       "  (12, '59871'),\n",
       "  (11, '59870'),\n",
       "  (7, '60210'),\n",
       "  (6, '59913')],\n",
       " [(9, '10011'),\n",
       "  (7, '59913'),\n",
       "  (5, '59850'),\n",
       "  (5, '10083'),\n",
       "  (4, '10089'),\n",
       "  (3, '59874'),\n",
       "  (3, '59873'),\n",
       "  (3, '10074'),\n",
       "  (3, '10066'),\n",
       "  (2, '59908')],\n",
       " [(10, '59849'),\n",
       "  (6, '59850'),\n",
       "  (6, '51151'),\n",
       "  (4, '51211'),\n",
       "  (3, '60213'),\n",
       "  (3, '60152'),\n",
       "  (3, '59905'),\n",
       "  (3, '59874'),\n",
       "  (2, '60187'),\n",
       "  (2, '60103')],\n",
       " [(23, '102610'),\n",
       "  (17, '101666'),\n",
       "  (7, '102591'),\n",
       "  (6, '102647'),\n",
       "  (6, '102604'),\n",
       "  (5, '102627'),\n",
       "  (5, '102609'),\n",
       "  (4, '102648'),\n",
       "  (4, '102608'),\n",
       "  (4, '102598')]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the top 10 similar documents for each query using dot product as similarity\n",
    "ans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0.561, '51060'),\n",
       "  (0.275, '51144'),\n",
       "  (0.24, '51120'),\n",
       "  (0.231, '51164'),\n",
       "  (0.217, '51135'),\n",
       "  (0.21, '51158'),\n",
       "  (0.201, '51184'),\n",
       "  (0.194, '51171'),\n",
       "  (0.178, '51130'),\n",
       "  (0.173, '51161')],\n",
       " [(0.266, '59905'),\n",
       "  (0.251, '60103'),\n",
       "  (0.225, '60170'),\n",
       "  (0.22, '60210'),\n",
       "  (0.21, '59850'),\n",
       "  (0.151, '60195'),\n",
       "  (0.15, '59909'),\n",
       "  (0.131, '60198'),\n",
       "  (0.129, '59870'),\n",
       "  (0.128, '60200')],\n",
       " [(0.224, '10064'),\n",
       "  (0.173, '10083'),\n",
       "  (0.171, '10063'),\n",
       "  (0.135, '10089'),\n",
       "  (0.132, '10052'),\n",
       "  (0.129, '10013'),\n",
       "  (0.113, '10066'),\n",
       "  (0.091, '59913'),\n",
       "  (0.081, '10067'),\n",
       "  (0.076, '101639')],\n",
       " [(0.112, '102656'),\n",
       "  (0.105, '102660'),\n",
       "  (0.098, '51207'),\n",
       "  (0.093, '51151'),\n",
       "  (0.093, '10027'),\n",
       "  (0.087, '59849'),\n",
       "  (0.083, '60213'),\n",
       "  (0.078, '102675'),\n",
       "  (0.066, '51206'),\n",
       "  (0.066, '102626')],\n",
       " [(0.392, '102598'),\n",
       "  (0.374, '102610'),\n",
       "  (0.277, '102647'),\n",
       "  (0.231, '101666'),\n",
       "  (0.22, '102609'),\n",
       "  (0.205, '100521'),\n",
       "  (0.192, '102617'),\n",
       "  (0.173, '102608'),\n",
       "  (0.164, '102634'),\n",
       "  (0.161, '102622')]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the top 10 similar documents for each query using cosine similarity\n",
    "ans2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geopandas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e63f9bed4f832b6a57923ae3f965cf33be10e8b918a73a0326339584b9798da4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
